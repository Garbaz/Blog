# System 1 and System 2 in AI

On the spectrum from the low level control of individual muscles in ones fingers, to philosophical deliberations about the meaning of life, there appears to be a discontinuity somewhere in the middle, at least in human cognition. There have been many attempts to somehow put a theoretical framework around this diffuse phenomenon, and many terms invented to describe the things above and below the jump. Here I will use the terms invented and popularized by [Kahnemann](https://en.wikipedia.org/wiki/Daniel_Kahneman), [*System 1* and *System 2*](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow).


## System 2

Under System 2 falls anything that we can reflect on, that we can deliberate with and put into words, where we can weigh options and make complex plans. In a sense, it is the part of our cognition that we tend to mean when we talk about "us", our selves.


## System 1

Under System 1 falls everything else, what we call our "intuition", "reflexes", "feelings". It's the part of our cognition that we tend to have little control over, that we can't really observe. It is, from the perspective from our selves, much closer to the physical properties of ones knee joint, than the abstract world of our thoughts. We don't tend to even notice all the complex things that go on in System 1, at least until something goes wrong.


## Symbolic AI

Much of classical AI was focused on trying to develop systems that implement the capabilities of System 2. After all, it's the part of our cognition that we are able to observe in ourselves. It's the part that we assign desireable qualities like "intelligence" or "rationality" to. The problem is, System 2 is nothing without System 1. Sure, we can build clever systems for logical deduction or semantic association, but the symbols that we are shuffling around mean very little without their connection to physical occurrences and sensory phenomena, especially when you try to build machines that actually do something in the real world.


## Large Language Models

While going against some traditions of symbolic AI, LLMs ultimately still largely only implement what we would classify as System 2 cognition. And that is not simply because they are based purely on language data, rather it is that they operate in an entirely discrete world, the world of tokens. While their rules for comprehending and generating these tokens are entirely stochastic, much to their success and the chagrin of many a linguist, 